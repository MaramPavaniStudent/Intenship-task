# -*- coding: utf-8 -*-
"""IMBD hugging face

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wQIUUKRF7tHUXvCJSNPEsT92Q0qiw3aO
"""

from datasets import load_dataset
from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments
import numpy as np
from sklearn.metrics import classification_report

from huggingface_hub import login
login("**********************")

!pip install fsspec==2023.6.0 --quiet

# Load the IMDb dataset
from datasets import load_dataset
dataset = load_dataset("imdb")

# Initialize the tokenizer
tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")

# Tokenization and preprocessing function
def preprocess_function(examples):
    return tokenizer(examples['text'], padding="max_length", truncation=True)

# Apply preprocessing
encoded_dataset = dataset.map(preprocess_function, batched=True)

# Load the model
model = DistilBertForSequenceClassification.from_pretrained("distilbert-base-uncased", num_labels=2)

pip install transformers --upgrade

import transformers
print(transformers.__version__)
print(transformers.__file__)

# Set training arguments
training_args = TrainingArguments(
    output_dir="./results",
    eval_strategy="epoch",
    learning_rate=5e-5,
    per_device_train_batch_size=16,
    num_train_epochs=3,
    weight_decay=0.01,
    logging_dir="./logs",
    logging_steps=10,
)

# Initialize the Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=encoded_dataset['train'],
    eval_dataset=encoded_dataset['test'],
)

# Start training
trainer.train()

# Evaluate the model
predictions = trainer.predict(encoded_dataset['test'])
predicted_labels = np.argmax(predictions.predictions, axis=1)
true_labels = encoded_dataset['test']['label']

# Generate a classification report
print(classification_report(true_labels, predicted_labels))